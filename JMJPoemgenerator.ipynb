{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","mount_file_id":"1AEy4Qmvql5M7cck1DfBrSolsBG9ap2e5","authorship_tag":"ABX9TyMbvP+TUNSz7dNNmh8NQj0a"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import numpy as np\n","from keras.models import Sequential\n","from keras.layers import LSTM, Dense, Embedding\n","from keras.optimizers import RMSprop\n","from keras.utils import to_categorical\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.callbacks import EarlyStopping"],"metadata":{"id":"DHYI5dNj0mri"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the data\n","with open('/content/drive/MyDrive/Colab Notebooks/modified_poems.txt', 'r') as file:\n","    text = file.read().lower()\n","\n","# Tokenize the data\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts([text])\n","\n","# Convert text to sequences of integer values\n","sequences = tokenizer.texts_to_sequences([text])[0]\n","\n","# Prepare the dataset of input to output pairs encoded as integers\n","seq_length = 40\n","dataX = []\n","dataY = []\n","for i in range(0, len(sequences) - seq_length, 1):\n","    seq_in = sequences[i:i + seq_length]\n","    seq_out = sequences[i + seq_length]\n","    dataX.append([int(char) for char in seq_in])\n","    dataY.append(int(seq_out))\n","\n","# Total number of unique words\n","num_words = len(tokenizer.word_index) + 1\n","\n","# Prepare the dataset of input to output pairs encoded as integers\n","x = np.array(dataX)\n","y = to_categorical(dataY, num_classes=num_words)\n"],"metadata":{"id":"VWzLrqbT0q2-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XT_O0kxUQ2ko","executionInfo":{"status":"ok","timestamp":1686357527406,"user_tz":-120,"elapsed":421113,"user":{"displayName":"Asterbu Creations","userId":"06874830667264696425"}},"outputId":"9e737ca7-a00e-4df4-fdaa-ac06ef209b7d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/75\n","173/173 [==============================] - 9s 37ms/step - loss: 6.3843 - val_loss: 6.3667\n","Epoch 2/75\n","173/173 [==============================] - 6s 32ms/step - loss: 6.0687 - val_loss: 6.2858\n","Epoch 3/75\n","173/173 [==============================] - 6s 32ms/step - loss: 5.8604 - val_loss: 6.2352\n","Epoch 4/75\n","173/173 [==============================] - 6s 33ms/step - loss: 5.6996 - val_loss: 6.1054\n","Epoch 5/75\n","173/173 [==============================] - 6s 33ms/step - loss: 5.5506 - val_loss: 6.0376\n","Epoch 6/75\n","173/173 [==============================] - 6s 33ms/step - loss: 5.4175 - val_loss: 6.0746\n","Epoch 7/75\n","173/173 [==============================] - 6s 32ms/step - loss: 5.2960 - val_loss: 5.9892\n","Epoch 8/75\n","173/173 [==============================] - 6s 32ms/step - loss: 5.1808 - val_loss: 5.9740\n","Epoch 9/75\n","173/173 [==============================] - 6s 32ms/step - loss: 5.0509 - val_loss: 6.0440\n","Epoch 10/75\n","173/173 [==============================] - 5s 32ms/step - loss: 4.9357 - val_loss: 6.0529\n","Epoch 11/75\n","173/173 [==============================] - 6s 32ms/step - loss: 4.8155 - val_loss: 6.2402\n","Epoch 12/75\n","173/173 [==============================] - 6s 32ms/step - loss: 4.6992 - val_loss: 6.1519\n","Epoch 13/75\n","173/173 [==============================] - 6s 32ms/step - loss: 4.5963 - val_loss: 6.2772\n","Epoch 14/75\n","173/173 [==============================] - 6s 32ms/step - loss: 4.4563 - val_loss: 6.2872\n","Epoch 15/75\n","173/173 [==============================] - 6s 32ms/step - loss: 4.3373 - val_loss: 6.4739\n","Epoch 16/75\n","173/173 [==============================] - 6s 32ms/step - loss: 4.2049 - val_loss: 6.4107\n","Epoch 17/75\n","173/173 [==============================] - 6s 32ms/step - loss: 4.0772 - val_loss: 6.5228\n","Epoch 18/75\n","173/173 [==============================] - 6s 32ms/step - loss: 3.9365 - val_loss: 6.8311\n","Epoch 19/75\n","173/173 [==============================] - 6s 32ms/step - loss: 3.7954 - val_loss: 6.9481\n","Epoch 20/75\n","173/173 [==============================] - 6s 32ms/step - loss: 3.6430 - val_loss: 7.0395\n","Epoch 21/75\n","173/173 [==============================] - 5s 32ms/step - loss: 3.5059 - val_loss: 7.2161\n","Epoch 22/75\n","173/173 [==============================] - 6s 32ms/step - loss: 3.3451 - val_loss: 7.2610\n","Epoch 23/75\n","173/173 [==============================] - 6s 32ms/step - loss: 3.2029 - val_loss: 7.3748\n","Epoch 24/75\n","173/173 [==============================] - 6s 32ms/step - loss: 3.0620 - val_loss: 7.5240\n","Epoch 25/75\n","173/173 [==============================] - 6s 32ms/step - loss: 2.9104 - val_loss: 7.6327\n","Epoch 26/75\n","173/173 [==============================] - 6s 32ms/step - loss: 2.7396 - val_loss: 7.8572\n","Epoch 27/75\n","173/173 [==============================] - 6s 32ms/step - loss: 2.5858 - val_loss: 8.1492\n","Epoch 28/75\n","173/173 [==============================] - 5s 32ms/step - loss: 2.4390 - val_loss: 7.9834\n","Epoch 29/75\n","173/173 [==============================] - 6s 32ms/step - loss: 2.3076 - val_loss: 8.1155\n","Epoch 30/75\n","173/173 [==============================] - 6s 32ms/step - loss: 2.2046 - val_loss: 8.2835\n","Epoch 31/75\n","173/173 [==============================] - 6s 32ms/step - loss: 2.0729 - val_loss: 8.3328\n","Epoch 32/75\n","173/173 [==============================] - 6s 32ms/step - loss: 1.9647 - val_loss: 8.4334\n","Epoch 33/75\n","173/173 [==============================] - 6s 32ms/step - loss: 1.8262 - val_loss: 8.5122\n","Epoch 34/75\n","173/173 [==============================] - 6s 32ms/step - loss: 1.6949 - val_loss: 8.7447\n","Epoch 35/75\n","173/173 [==============================] - 6s 32ms/step - loss: 1.5763 - val_loss: 8.7507\n","Epoch 36/75\n","173/173 [==============================] - 6s 32ms/step - loss: 1.4737 - val_loss: 8.6927\n","Epoch 37/75\n","173/173 [==============================] - 6s 32ms/step - loss: 1.3567 - val_loss: 8.9063\n","Epoch 38/75\n","173/173 [==============================] - 6s 32ms/step - loss: 1.2594 - val_loss: 9.0902\n","Epoch 39/75\n","173/173 [==============================] - 6s 32ms/step - loss: 1.1591 - val_loss: 9.2882\n","Epoch 40/75\n","173/173 [==============================] - 6s 32ms/step - loss: 1.0845 - val_loss: 9.2345\n","Epoch 41/75\n","173/173 [==============================] - 6s 32ms/step - loss: 0.9951 - val_loss: 9.2435\n","Epoch 42/75\n","173/173 [==============================] - 6s 32ms/step - loss: 0.9242 - val_loss: 9.2166\n","Epoch 43/75\n","173/173 [==============================] - 6s 32ms/step - loss: 0.8698 - val_loss: 9.4657\n","Epoch 44/75\n","173/173 [==============================] - 6s 32ms/step - loss: 0.7983 - val_loss: 9.5941\n","Epoch 45/75\n","173/173 [==============================] - 6s 32ms/step - loss: 0.7517 - val_loss: 9.7551\n","Epoch 46/75\n","173/173 [==============================] - 5s 32ms/step - loss: 0.7074 - val_loss: 9.5627\n","Epoch 47/75\n","173/173 [==============================] - 6s 32ms/step - loss: 0.6698 - val_loss: 9.7020\n","Epoch 48/75\n","173/173 [==============================] - 6s 32ms/step - loss: 0.6316 - val_loss: 9.6375\n","Epoch 49/75\n","173/173 [==============================] - 6s 33ms/step - loss: 0.5877 - val_loss: 9.9109\n","Epoch 50/75\n","173/173 [==============================] - 6s 32ms/step - loss: 0.5610 - val_loss: 9.7679\n","Epoch 51/75\n","173/173 [==============================] - 6s 32ms/step - loss: 0.5223 - val_loss: 10.0436\n","Epoch 52/75\n","173/173 [==============================] - 6s 32ms/step - loss: 0.5065 - val_loss: 10.2844\n","Epoch 53/75\n","173/173 [==============================] - 5s 32ms/step - loss: 0.4557 - val_loss: 10.1970\n","Epoch 54/75\n","173/173 [==============================] - 6s 32ms/step - loss: 0.4390 - val_loss: 10.2751\n","Epoch 55/75\n","173/173 [==============================] - 6s 32ms/step - loss: 0.4083 - val_loss: 10.2197\n","Epoch 56/75\n","173/173 [==============================] - 6s 32ms/step - loss: 0.3814 - val_loss: 10.1124\n","Epoch 57/75\n","173/173 [==============================] - 6s 32ms/step - loss: 0.3667 - val_loss: 10.2873\n","Epoch 58/75\n","173/173 [==============================] - 6s 32ms/step - loss: 0.3536 - val_loss: 10.3442\n","Epoch 59/75\n","173/173 [==============================] - 6s 32ms/step - loss: 0.3435 - val_loss: 10.2308\n","Epoch 60/75\n","173/173 [==============================] - 6s 32ms/step - loss: 0.3320 - val_loss: 10.3821\n","Epoch 61/75\n","173/173 [==============================] - 6s 32ms/step - loss: 0.3053 - val_loss: 10.3863\n","Epoch 62/75\n","173/173 [==============================] - 5s 32ms/step - loss: 0.2957 - val_loss: 10.5017\n","Epoch 63/75\n","173/173 [==============================] - 6s 32ms/step - loss: 0.2892 - val_loss: 10.6037\n","Epoch 64/75\n","173/173 [==============================] - 6s 32ms/step - loss: 0.2622 - val_loss: 10.5216\n","Epoch 65/75\n","173/173 [==============================] - 6s 32ms/step - loss: 0.2682 - val_loss: 10.7341\n","Epoch 66/75\n","173/173 [==============================] - 6s 32ms/step - loss: 0.2586 - val_loss: 10.7168\n","Epoch 67/75\n","173/173 [==============================] - 6s 32ms/step - loss: 0.2421 - val_loss: 10.7550\n","Epoch 68/75\n","173/173 [==============================] - 6s 32ms/step - loss: 0.2310 - val_loss: 10.6728\n","Epoch 69/75\n","173/173 [==============================] - 6s 32ms/step - loss: 0.2258 - val_loss: 10.6165\n","Epoch 70/75\n","173/173 [==============================] - 6s 32ms/step - loss: 0.2181 - val_loss: 10.8879\n","Epoch 71/75\n","173/173 [==============================] - 6s 32ms/step - loss: 0.2092 - val_loss: 10.3334\n","Epoch 72/75\n","173/173 [==============================] - 6s 32ms/step - loss: 0.2094 - val_loss: 10.5626\n","Epoch 73/75\n","173/173 [==============================] - 6s 32ms/step - loss: 0.2128 - val_loss: 10.8091\n","Epoch 74/75\n","173/173 [==============================] - 6s 32ms/step - loss: 0.2038 - val_loss: 10.9434\n","Epoch 75/75\n","173/173 [==============================] - 6s 32ms/step - loss: 0.1990 - val_loss: 10.7051\n"]}],"source":["# Build the model\n","model = Sequential()\n","model.add(Embedding(input_dim=num_words, output_dim=256, input_length=seq_length))\n","model.add(LSTM(400, return_sequences=True))\n","model.add(LSTM(400))\n","model.add(Dense(num_words, activation='softmax'))\n","\n","\n","# Compile the model\n","optimizer = RMSprop(learning_rate=0.01)\n","model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n","\n","\n","\n","# Prepare the callback\n","early_stopping = EarlyStopping(monitor='loss', patience=4)  # Stop if loss doesn't improve for 4 consecutive epochs\n","\n","# Add it to the `fit` method\n","model.fit(x, y, batch_size=50, epochs=75, callbacks=[early_stopping], validation_split=0.1)\n","\n","\n","model.save('modelword3.h5')\n"]},{"cell_type":"code","source":["from keras.models import load_model\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","\n","def generate_seq(model, tokenizer, seed_text, seq_length, n_words):\n","    result = list()\n","    in_text = seed_text\n","    # generate a fixed number of words\n","    for _ in range(n_words):\n","        # encode the text as integer\n","        encoded = tokenizer.texts_to_sequences([in_text])[0]\n","        # truncate sequences to a fixed length\n","        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n","        # predict probabilities for each word\n","        yhat = model.predict(encoded, verbose=0).argmax(axis=-1)\n","        # map predicted word index to word\n","        out_word = ''\n","        for word, index in tokenizer.word_index.items():\n","            if index == yhat:\n","                out_word = word\n","                break\n","        # append to input\n","        in_text += ' ' + out_word\n","        result.append(out_word)\n","    return ' '.join(result)\n","\n","# load the model\n","model = load_model('/content/drive/MyDrive/Colab Notebooks/modelword4.h5')\n","\n","# specify the seed text and the length of the generated sequence\n","seed_text = 'India '\n","seq_length = 40  # should be the same as seq_length during training\n","n_words = 22  # number of words to generate\n","\n","# generate new text\n","generated = generate_seq(model, tokenizer, seed_text, seq_length, n_words)\n","print(generated)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t14ndZhYU7Ic","executionInfo":{"status":"ok","timestamp":1686362556698,"user_tz":-120,"elapsed":3418,"user":{"displayName":"Asterbu Creations","userId":"06874830667264696425"}},"outputId":"a28aa67f-f14f-4b47-db7a-a7ca51b639bf"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["by love's light amidst the waves where life may brew our love is deep as time we are my strength is unfurled\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"S4DwMj6vVcUq"}},{"cell_type":"code","source":[],"metadata":{"id":"DdoZNZ4wWKUX"},"execution_count":null,"outputs":[]}]}